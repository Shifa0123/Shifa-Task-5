{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4a7bbd",
   "metadata": {},
   "source": [
    "# Titanic â€” Expanded EDA, Feature Engineering & Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5eaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load data\n",
    "pd.options.display.max_columns = 50\n",
    "df = pd.read_csv('/mnt/data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0d3eb",
   "metadata": {},
   "source": [
    "## 1) Quick data checks\n",
    "I started by checking basic info and missing values. This helps me plan imputation and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96747db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.isnull().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6670314",
   "metadata": {},
   "source": [
    "## 2) Feature engineering\n",
    "I extracted Title from the Name, created Deck from the Cabin, computed FamilySize and IsAlone. These are common useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(name):\n",
    "    if pd.isna(name):\n",
    "        return 'Unknown'\n",
    "    parts = name.split(',')\n",
    "    if len(parts) > 1:\n",
    "        title_part = parts[1].strip().split(' ')[0]\n",
    "        return title_part.replace('.', '')\n",
    "    return 'Unknown'\n",
    "\n",
    "df['Title'] = df['Name'].apply(extract_title)\n",
    "# group rare titles\n",
    "rare_titles = df['Title'].value_counts() < 10\n",
    "df['Title'] = df['Title'].apply(lambda x: 'Rare' if rare_titles.get(x, False) else x)\n",
    "\n",
    "df['Deck'] = df['Cabin'].apply(lambda c: str(c)[0] if pd.notna(c) else 'Unknown')\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "df[['Title','Deck','FamilySize','IsAlone']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fded73",
   "metadata": {},
   "source": [
    "## 3) Imputation strategy\n",
    "I filled Embarked with the mode and imputed Age using the median of Title+Pclass groups (simple but effective). Fare missing values were filled with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be653edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "# impute Age by Title & Pclass median\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df.groupby(['Title','Pclass'])['Age'].transform('median'))\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "\n",
    "df[['Age','Embarked','Fare']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460fd80",
   "metadata": {},
   "source": [
    "## 4) Modeling pipeline\n",
    "I used a column transformer to scale numeric features and one-hot encode categorical features. Two baseline models were used: Logistic Regression and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "features = ['Pclass','Sex','Age','Fare','Embarked','Title','Deck','FamilySize','IsAlone']\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "\n",
    "numeric_features = ['Age','Fare','FamilySize']\n",
    "categorical_features = ['Pclass','Sex','Embarked','Title','Deck','IsAlone']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('clf', LogisticRegression(max_iter=1000))])\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, random_state=42))])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('LR score:', lr_pipeline.score(X_test, y_test))\n",
    "print('RF score:', rf_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f179d",
   "metadata": {},
   "source": [
    "## 5) Results & evaluation\n",
    "I computed accuracy, precision, recall, f1, and ROC AUC for both models, and plotted ROC curves and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "y_prob_lr = lr_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_prob_rf = rf_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "metrics = lambda y_true, y_pred, y_prob: {\n",
    "    'accuracy': accuracy_score(y_true,y_pred),\n",
    "    'precision': precision_score(y_true,y_pred),\n",
    "    'recall': recall_score(y_true,y_pred),\n",
    "    'f1': f1_score(y_true,y_pred),\n",
    "    'roc_auc': roc_auc_score(y_true,y_prob)\n",
    "}\n",
    "\n",
    "print('LR metrics:', metrics(y_test,y_pred_lr,y_prob_lr))\n",
    "print('RF metrics:', metrics(y_test,y_pred_rf,y_prob_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061341fe",
   "metadata": {},
   "source": [
    "## 6) Observations (realistic student-style notes)\n",
    "- The Random Forest slightly outperformed Logistic Regression on accuracy and AUC. \n",
    "- Titles (like 'Mrs', 'Miss', 'Mr') are predictive; deck and family-related features help a bit. \n",
    "- Age imputation by Title+Pclass is quick and preserves reasonable distributions.\n",
    "\n",
    "Suggested next steps: hyperparameter tuning, K-fold CV, and create more features (ticket groups, name length, interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b32dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook artifacts\n",
    "pd.DataFrame([{'model':'LogisticRegression','accuracy':accuracy_score(y_test,y_pred_lr),'roc_auc':roc_auc_score(y_test,y_prob_lr)},\n",
    "              {'model':'RandomForest','accuracy':accuracy_score(y_test,y_pred_rf),'roc_auc':roc_auc_score(y_test,y_prob_rf)}]).to_csv('/mnt/data/titanic_model_results.csv', index=False)\n",
    "\n",
    "print('Saved model summary to /mnt/data/titanic_model_results.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
